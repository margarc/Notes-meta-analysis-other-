#Notes from: "Tutorial on meta-analysis in R"
# Fixed -> Same mean ES, zero between-study variance
# Fixed effect model: The observed effects are sampled from a distribution with true effect μ, and variance σ2. The observed effect
# T1 is equal to μ+εi (Borenstein et al 2007) 
#
# Random effects: Random effects model. The observed effect T1 (box) is sampled from a distribution with true effect θ1, and variance σ2
# This true effect θ1, in turn, is sampled from a distribution with mean μ and variance τ 2 (Borenstein et al 2007). 
#
# Random -> Different mean ES, between-study variance
# ES= effect size
#The Random effects model regards the studies as a sample of a larger universe of studies.
#The Random effects model can be used to infer what would likely happen if a new study were performed, the FE model cannot.
# Common practice is to report both fixed and random effects model results.
#
# "Under the random effects model we allow that the true effect could vary from study to study. 
# For example, the effect size might be a little higher if the subjects are older, or more educated, or healthier; or if the study used a
# slightly more intensive or longer variant of the intervention; or if the effect was measured more reliably; and so on. 
# The studies included in the meta-analysis are assumed to be a random sample of the relevant distribution of effects, and
# the combined effect estimates the mean effect in this distribution (Borenstein et al 2007) "
#
# In the REM: Boxes for the large studies have decreased area while those for the small studies such as have increased area. As we move from fixed effect to random effects,
# extreme studies will lose influence if they are large, and will gain influence if they are small. (Borenstein et al 2007)
#
# "As compared with the fixed effect model, the weights assigned under random effects are more balanced. Large studies are less likely
# to dominate the analysis and small studies are less likely to seem less important."
# in our case we cannot assume a common effect size cz studies differ in various ways. Results generalisable to a range of populations 
# not just one specific. 
#  [ larger sample size: often the easiest way to boost the statistical power of a test]
-------------------------------------------------------------------------------------------------------------------------------
# Reminder of Type I vs Type II errors
# Type I error: Incorrect rejection of the null hypothesis, despite it being true (false +ve)
if the level of significance is 0.05, there is a 5% chance a type I error may occur.

# Type II error: Accepting the null hypothesis when actually it is false.
Probability of committing a type II error is equal to the power of the test, also known as beta. 
The power of the test could be increased by increasing the sample size, which decreases the risk of committing a type II error.




------------------------------------------------------------------------------------------------------------------------------------
The I squared statistic is a description of the proportion of total variation in study estimates due to
heterogeneity (Higgins, 2002).
Also consider size & direction of effects 
Forest plot: Larger boxes show a study that has higher precision, and greater weight
